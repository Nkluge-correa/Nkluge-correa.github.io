---
title: "TeenyTinyLlama: open-source, tiny, and totally Brazilian"
excerpt: "TeenyTinyLlama is an open‑source initiative dedicated to developing compact yet capable foundational language models trained natively in Brazilian Portuguese.<br><br><img src='https://raw.githubusercontent.com/Nkluge-correa/TeenyTinyLlama/refs/heads/main/img/combined-logo.png' width='400'>"
collection: portfolio
---

| [Hugging Face](https://huggingface.co/collections/nicholasKluge/teenytinyllama-6582ea8129e72d1ea4d384f1) | [Preprint](https://arxiv.org/abs/2401.16640) | [Paper](https://www.sciencedirect.com/science/article/pii/S2666827024000343?via%3Dihub) | [Demo](https://huggingface.co/spaces/nicholasKluge/TeenyTinyLlama-Chat) |

TeenyTinyLlama is a lovingly engineered family of tiny-but-mighty open-source language models trained natively in Brazilian Portuguese. Designed for researchers, tinkerers, and anyone exploring LLMs beyond the English-centric world, TeenyTinyLlama shows how far small models can go when optimized for a specific linguistic ecosystem.

Built entirely on open tooling, the project charts the whole journey of developing foundation models for low-resource languages—from tokenizer training to large-scale pre-training, evaluation, and fine-tuning—while keeping the entire pipeline transparent, reproducible, and beautifully lightweight. Despite their size, these models pack a surprising punch, offering a controlled research testbed for studying multilinguality, bias, hallucinations, and efficiency constraints in language modeling.
